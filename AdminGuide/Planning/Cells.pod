
=head1 Designing an EFS Cell Namespace

Before one can install and configure your first EFS cell, some thought
has to be given to the namespace to be used for the cell names.  EFS
cell names are not ad-hoc, but are derived from a well defined four
level hierarchy of the root domain, region, campus and location.  This
namespace is the foundation for managing your global EFS
infrastructure, and once chosen, the names will be difficult to
change.

=head2 Root Domain

This is the root DNS domain name under which your entire EFS
infrastructure will be configured.  This should represent a single,
unified administrative domain.  Since EFS will be sharing data over
NFS among the EFS clients of the various cells, it is important that
this domain represent a single unified UNIX uid/gid namespace as well.

If you do not have an integrated, centrally managed uid/gid namespace,
then you should seriously consider creating multiple independent EFS
domains.  EFS can only be as secure as the underlying NFS
infrastructure on which it is based.

The root domain will typically be the top level domainname of your
organization, for example:

    bigbank.com
    foo.org

If you are creating an EFS infrastructure for a subdomain of your
organization, then you might use a subdomain like:

    engineering.bigbank.com

Remember that EFS supports both development and production cells, and
there is minimal support for inter-domain sharing of information, so
we recommend creating a single domain that can support your entire
organization.

The rest of these examples in this document will assume a root domain
of "example.com".

=head2 Region Names

The regions are the first sub domain of the root, and they function to
group the campuses together.  The region can be geographic or
organizational, as fits the support model of your organization.  Bear
in mind that campuses group together physical locations (data
centers), so which ever convention is used, the region is a
fundamentally geographic concept.

For example one of the possible conventions for region names would be
geographic continent names:

    Region  Description
    ------  -----------
    na      North America
    sa      South America
    eu      Europe
    af      Africa
    as      Asia
    au      Australia
    an      Antarctica

Another example might be top level organizational names, for example,
the region conventions used at the original EFS developer's employer
are:

    Region  Description
    ------  -----------
    amrs    Americas
    emea    Europe, Middle East, Africa
    aja     Asia, Japan, Australia[*]

Choose these names carefully, as they will be very difficult to change.

[*] Yes, we know that Japan is part of Asia. 

=head2 Campus Names

A campus is the second subdomain, and they function to group together
locations, which are essentially data centers.  Campuses are intended
to represent collections of locations which serve a single MAN
(metropolitan area network).  We recommend that campus names be based
on the main city around which a given MAN is based, even if the MAN
spans multiple cities.  The naming convention we recommend is to use
the UN/LOCODE identifier for the primary city, without the country
code.  If conflicts arise as a result of dropping the country code,
then simply choose a reasonable alternative as an exception.

As a reference, the UN/LOCODE database can be found here:

    http://www.unece.org/cefact/locode/service/location.htm

For example, consider a large MAN that spans New York City, as well as
data centers in New Jersey, Pennsylvania, and maybe even other nearby
states.  The central city in this case is NYC, so we would recommend
using "nyc" as the campus name.

Some example campus names would be:

    Campus  Description
    ------  -----------
    nyc     New York City, NY, USA
    lon     London, England
    tyo     Tokyo, Japan
    hkg     Hong Kong
    sin     Singapore

=head2 Location Names

A location is essentially intended to represent a single data center.
Location names must be unique, even though they can be fully qualified
by the campus, region and root domain.  We recommend creating short 3
to 5 character names based on the most commonly known name of the data
center site, building, or address.  In most cases, we expect there to
be existing conventions to leverage.

Some example location names might be:

    Location  Description
    --------  -----------
    lvt       Levvitown, PA Data Center
    hqb       Head Quarters Building
    abc       Acme Banking Center

Remember that these names must be globally unique.

=head2 Cell Names

The final component of the fully qualified EFS cell name is used to
distinguish multiple cells in a single data center.  Strictly
speaking, EFS can support as many cells in a single data center as
needed, but we recommend only two: dev and prod.  A cell is the
smallest unit which can be updated by the EFS distribution mechanism,
and the strength of EFS is the ability to manage the entire dev and/or
prod environment centrally.  One could break down a single data center
into multiple prod cells, or even multiple dev cells, but this is not
how we intended it to be used.   YMMV applies, as usual.

Our recommendation is therefore to keep the cell names short and
simple, and to use a single letter (d for dev, and p for prod) for the
final cell name component.

For purposes of illustration, we'll assume that continents have been
chosen for the region names.  Then, some sample cell names might be:

    d.abc.nyc.na.example.com
    p.def.lon.eu.example.com
    p.ghi.tyo.as.example.com

=head1 Relationship Between Cell Names and DNS

One thing that should be obvious at this point, since we based this
hierarchy on a DNS domainname that is almost certainly already used to
manage the hostnames for your organization, is that these cell names,
and the location/campus/region hierarchy seems to imply a specific DNS
domain structure.

It does, but strictly speaking, there is no requirement that the same
multi-level hierarchy be implemented in your DNS namespace.
Obviously, if you have an existing multi-level hierarchy similar to
this, it would make sense to leverage it.

EFS assumes these names represent the DNS namespace in exactly one
place: efsclient, the client side boot script that creates the EFS NFS
mounts, dynamically at boot time.  The EFS client can be configured
with just two pieces of information: the EFS cell name, and the EFS
client hosttype.  The client searches the DNS namespace for the NFS
server for it's cell using these names, and through the clever use of
CNAME records, the requirement for creating this entire multi-level
namespace in DNS can be made optional.

See the EFS client documentation for more information.

=head1 Server Infrastructure

=head2 NFS Servers

Each EFS cell requires a single NFS server dedicated for that cell.
The NFS server can be used to serve non-EFS data, on volumes other
than those required by EFS, but each NFS server must be defined for
one, and only one EFS cell.  You will need one at least one NFS server
for each distinct EFS cell you want to deploy.

EFS will only be as stable as the NFS infrastructure on top of which
it is deployed, so these NFS servers should be configured using some
kind of clustering technology, should be backed up, etc. etc.  EFS
does B<NOT> manage the NFS servers themselves, rather it manages the
content of the data served via the EFS volumes on those servers.

In the future, we would like to extend EFS to manage as much of the
NFS server configuration as possible, but the current implementation
has no such support.  As we work to extend EFS to support both NFSv4
and OpenAFS, we hope to add more direct support for managing the
backend fileserver infrastructure, not just the content, as we do
today.

=head3 Scale Out Strategies for NFS

An EFS cell is built on top of a single NFS server, and the EFS
software manages content for that cell (either in /efs/dev or
/efs/dist) by managing the contents of that one fileserver.  While
modern NAS filers can support large numbers of NFS clients, NFS
doesn't scale infinitely, and yet we're still recommending one dev,
and/or one prod cell per data center.

How do we make this work?  EFS 3 still has no support for multiple NFS
servers in a single cell, so how do you manage a large number of
clients in a large corporate data center?  The NFS server design for
the propietary EFS 2 environment can serve as a model for NFS scale
out.

Each EFS cell is defined with a single NFS server, and when EFS
manages content, it does so by managing the contents of the EFS
volumes on that one fileserver.  In EFS 2, these servers are all
NetApp NAS filers, and we are heavily leveraging their FlexCache
technology to manage the client/server ratio in the larger data
centers.

For example, a typical installation will actually be one "read-write"
NAS filer, which hosts the writable volumes.  This is the NAS filer
defined in the EFS database for the cell.  That data center will also
have several additional NAS filers, each of which also server the
/efs/dist volume, but on these filers, that volume is defined as a
FlexCache for the original, read-write volume for /efs/dist.

When EFS updates the read-write volume, on just the one NAS filer, the
updates are transparently made available on the other NAS filers, so
effectively, this provides an automated "intra-cell" distribution
mechanism.  Clients are then balanced across the various NAS filers,
all of which have the same, identical set of data in /efs/dist, using
DNS round robin CNAME records, or in some cases (sadly) manually
configuring the clients.

None of this is automated by EFS, since management of the NAS filers
is outside the scope of responsibility for the EFS team.
Strategically, we hope to work with the community of sites that have
adopted EFS to decide how to best extend EFS to support this backend
infrastructure, so that we can bring the same degree of scalability to
the NFS servers as EFS provides for content management.

There are several directions the code can take, and we are very
interested in pursuing NFSv4 and OpenAFS, and extending EFS to manage
far more of the underlying infrastructure than it does today.

=head2 EFS Servers

Each EFS cell will require a minimum of one EFS server on which the
EFS management software (efsd) will run.  Each EFS server can manage
multiple cells, and we recommend using at least four (4) EFS servers
to manage your cells.  Two of these will have a hosttype of "prod",
will run the prod release of efs/core, and two will have a hosttype of
"dev", and run the development release of efs/core.  See the "Managing
EFS Servers" documentation in the Admin Guide for more details.

In theory, you can use any supported EFS client platform, but the
majority of our development and testing has been on CentOS 5.4, on
x86_64.

We recommend restricting access to these machines, since they will
have full root access to the EFS volumes on your NFS servers, and if
root on these hosts is compromised, then the integrity of the contents
of EFS have been compromised.

The EFS server (efsd) does not consume a tremendous amount of
resources, since it is idle except when users are running efs client
commands.  The number of servers you need, and the capacity of these
servers (with respect to CPU and Memory, not disk space) depends on
the scale of your environment, and the frequency with which you expect
to update and distribute software.

=head3 Properly Defined Hostnames and Addresses

EFS security requires the use of SSL for server-server authentication
and client-server privacy, and this will only work if the EFS server's
hostnames and IP addresses are correctly defined in your naming
services.  The most common use-case is DNS, and that means that the
hostname used to refer to the server must map to one of it's valid IP
addresses, and more importantly, that the IP addresses used by the
server resolve to the host's fully qualified, canonical hostname.

If you are stuck in the 1980's, and not using DNS, then you can still
satisfy this requirement using /etc/hosts, or NIS, or any other
alternate mechanism, provided the names map to the correct addresses,
and the addresses map to the correct name.

Consult with the EFS Development team if you are having trouble
satisfying this basic requirement, and we may be able to recommend
domain-specific workarounds.

=cut    
